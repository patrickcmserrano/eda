# Wallet Core (EDA Study)

Este projeto Ã© uma implementaÃ§Ã£o de um nÃºcleo de transaÃ§Ãµes financeiras (Wallet Core) baseada em Arquitetura Orientada a Eventos (EDA).

Foi desenvolvido como parte do desafio do curso Full Cycle, mas utilizando uma stack moderna e funcional com **Clojure** e **Polylith**, focando em imutabilidade, modularidade e separation of concerns.

## ðŸ— Arquitetura & Stack

O projeto segue uma estrutura de Monorepo modular gerenciada pelo **Polylith**:

* **Linguagem:** Clojure (JDK 17+)
* **Gerenciamento de Workspace:** Polylith (`poly`)
* **ValidaÃ§Ã£o de Dados:** Malli
* **Banco de Dados:** PostgreSQL 15 (via `next.jdbc` e `HoneySQL`)
* **Mensageria:** Apache Kafka (via `Jackdaw`)
* **API:** Jetty + Reitit + Ring

### Estrutura do Workspace

```text
bases/
  â””â”€â”€ wallet-api/       # API Gateway (REST -> Componentes)
components/
  â”œâ”€â”€ account/          # DomÃ­nio de Contas e Saldo
  â”œâ”€â”€ client/           # DomÃ­nio de Clientes
  â”œâ”€â”€ transaction/      # Core: Atomicidade e OrquestraÃ§Ã£o
  â”œâ”€â”€ database/         # Infra: Connection Pool e Migrations
  â””â”€â”€ kafka/            # Infra: Producers
projects/
  â””â”€â”€ wallet/           # Artefato DeployÃ¡vel (Uberjar)

```

## ðŸš€ Como Rodar

### PrÃ©-requisitos

* Docker & Docker Compose
* Clojure CLI
* Ferramenta `poly` (opcional, mas recomendada)

### 1. Subir Infraestrutura

Inicie o PostgreSQL, Zookeeper e Kafka:

```bash
docker compose up -d

```

### 2. Rodar a AplicaÃ§Ã£o (Modo Dev)

VocÃª pode rodar diretamente via Clojure CLI a partir do projeto `wallet`:

```bash
cd projects/wallet
clojure -M -m br.com.eda.wallet-api.core

```

Ou, se preferir rodar tudo via Docker (Build Final):

```bash
docker compose up --build app

```

A API estarÃ¡ disponÃ­vel em: `http://localhost:8080`

## ðŸ§ª Testando a API

### Criar Cliente

```bash
curl -X POST http://localhost:8080/clients \
  -H "Content-Type: application/json" \
  -d '{"name": "Neo", "email": "neo@matrix.com"}'

```

### Criar Conta

Use o `id` retornado acima.

```bash
curl -X POST http://localhost:8080/accounts \
  -H "Content-Type: application/json" \
  -d '{"client_id": "UUID_DO_CLIENTE"}'

```

### Realizar TransaÃ§Ã£o

Isso debita da origem, credita no destino (Atomicamente) e publica no Kafka.

```bash
curl -X POST http://localhost:8080/transactions \
  -H "Content-Type: application/json" \
  -d '{
    "account_id_from": "UUID_CONTA_ORIGEM",
    "account_id_to": "UUID_CONTA_DESTINO",
    "amount": 1000
  }'

```

## ðŸ›  Desenvolvimento

Para rodar os testes de todos os componentes:

```bash
clojure -M:poly test

```

Para verificar a integridade do workspace:

```bash
clojure -M:poly check

```

---

## âš ï¸ Trade-offs e Melhorias Futuras

### ConsistÃªncia de Dados (The Dual Write Problem)
A implementaÃ§Ã£o atual do componente `Transaction` utiliza uma abordagem pragmÃ¡tica para o escopo deste exercÃ­cio:
1. Commit da transaÃ§Ã£o no PostgreSQL (Atomicidade garantida via `jdbc/with-transaction`).
2. PublicaÃ§Ã£o do evento no Kafka (Fire and forget).

**CenÃ¡rio de Risco:**
Existe uma janela de falha teÃ³rica (milissegundos) entre o commit do banco e a publicaÃ§Ã£o no Kafka. Se o processo da aplicaÃ§Ã£o for encerrado abruptamente (Crash/OOM/Falha de Rede) exatamente neste intervalo, o sistema entrarÃ¡ em estado inconsistente (Dinheiro debitado, mas evento nÃ£o emitido).

**SoluÃ§Ã£o para ProduÃ§Ã£o:**
Para evoluir este projeto para um ambiente crÃ­tico, a soluÃ§Ã£o recomendada seria implementar o **Transactional Outbox Pattern**:
1. Persistir o evento em uma tabela `outbox` dentro da mesma transaÃ§Ã£o SQL da transferÃªncia.
2. Utilizar um processo assÃ­ncrono (Relay ou CDC com Debezium) para ler a tabela `outbox` e publicar no Kafka com garantia de entrega *At-Least-Once*.

### Outras Melhorias
* **IdempotÃªncia no Consumo:** Garantir que os consumidores Kafka lidem com mensagens duplicadas.
* **Schema Registry:** Adotar Avro ou JSON Schema para contrato estrito de mensagens.
* **Distributed Tracing:** Implementar OpenTelemetry para rastrear o fluxo entre API -> DB -> Kafka.